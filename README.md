# Nano-vLLM Practice

My learning journey to understand LLM inference optimization.

## Roadmap
1. Study `nanoGPT` for Transformer basics.
2. Study `nano-vllm` for KV Cache and PagedAttention.
3. Implement my own version with custom CUDA kernels (FlashAttention).

## References
- [nanoGPT](https://github.com/karpathy/nanoGPT)
- [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm)
